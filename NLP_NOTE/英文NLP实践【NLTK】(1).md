# è‹±æ–‡NLPå®è·µã€NLTKã€‘(1)

åœ¨**æµ·å¤–èˆ†æƒ…ç›‘æ§**é¡¹ç›®çš„å¥‘æœºä¸‹ï¼Œé€šè¿‡ä¸æ–­çš„é‡åˆ°é—®é¢˜ï¼Œå†è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°äº†ä¸€äº›å¥½ç©çš„æ–‡æœ¬å¤„ç†æ–¹æ³•å’Œä¸€äº›å®ç”¨çš„å·¥å…·ã€‚æ‰€ä»¥å†™åœ¨è¿™é‡Œå½“ä½œåˆ†äº«çš„åŒæ—¶ï¼Œä¹Ÿå½“ä½œæ˜¯è‡ªå·±çš„ä¸€ä¸ªå¤‡å¿˜å½•ã€‚ ç”±äºæ¸¸æˆèŠå¤©æ˜¯ä¸€ä¸ªå¤šè¯­ç§çš„ç¯å¢ƒï¼Œé’ˆå¯¹ä¸åŒè¯­è¨€çš„å¤„ç†é€»è¾‘å’ŒæŠ€å·§ä¹Ÿä¸å°½ç›¸åŒã€‚ä»Šå¤©å…ˆæ€»ç»“çš„æ˜¯**è‹±æ–‡**ï¼Œåç»­ä¼šé™†é™†ç»­ç»­æ›´æ–°**ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©è¯­**ç­‰å…¶ä»–è¯­ç§ã€‚é€‰æ‹©çš„åˆ†æå·¥å…·æ˜¯`NLTK`ï¼ˆpsï¼š`NLTK`ä¸æ”¯æŒä¸­æ–‡ï¼‰ï¼Œä¸‹æ–‡ä¸­å±•ç¤ºçš„ä»£ç ä¾‹å­ä¹Ÿæ˜¯é€šè¿‡`NLTK`æ¥å®è·µçš„ã€‚

- [x] ğŸ§ **æ–‡æœ¬æŒ–æ˜çš„æ€è·¯**
- [x] ğŸ›  **æ–‡æœ¬é¢„å¤„ç†**ï¼›
- [x] ğŸš® **è¯­ç§åˆ¤åˆ«**ï¼›
- [x] ğŸ”ª **åˆ†å¥ & åˆ†è¯**ï¼›
- [x] ğŸŒŸ **è¯æ€§æ ‡æ³¨**ï¼›
- [x] ğŸ”· **åˆ†å—åˆ‡è¯**ï¼›
- [x] ğŸ‘¶ **è¯å½¢è¿˜åŸ & è¯å¹²æå–**ï¼›
- [x] â›³ï¸ **è¿‡æ»¤æçº¯**ï¼›

## æ–‡æœ¬æŒ–æ˜çš„æ€è·¯

åœ¨åˆ†æä¹‹å‰ï¼Œåº”å½“æœ‰æ„è¯†çš„ç»´æŠ¤ä¸€å¥—è‡ªå·±çš„**ç”¨æˆ·è¯å…¸**ï¼Œå¯ä»¥è®©è¿è¥åŒäº‹æä¾›æ¸¸æˆä¸“ä¸šåè¯ï¼Œè¿™äº›è¯ä¹Ÿå¾€å¾€æ˜¯è¿è¥åŒäº‹æ¯”è¾ƒå…³æ³¨çš„é‡ç‚¹è¯æ±‡ï¼Œè¿™äº›è¯æ±‡åœ¨å¸‚é¢ä¸Šçš„åˆ†è¯å·¥å…·ä¸­æ˜¯æ¯”è¾ƒéš¾å‡†ç¡®åˆ‡å‰²çš„ã€‚åŒæ—¶ï¼Œå¯»æ‰¾åˆé€‚çš„*stopwords***åœç”¨è¯è¯å…¸**ï¼Œå¯¹åç»­çš„åˆ†è¯ç»“æœå†åšä¸€å±‚è¿‡æ»¤ã€‚å­—å…¸æœ€å¥½æœ‰ä¸€ä¸ª**æ ‡å‡†åŒ–**çš„æ ¼å¼ï¼Œåç»­åæ–°çš„ç”¨é€”å¯ä»¥çµæ´»çš„ä½¿ç”¨ã€‚ç›®å‰æˆ‘çš„å­—å…¸æŒ‰ç…§ä¸‹é¢è¿™ä¸ªæ ¼å¼è¿›è¡Œæ•´ç†ï¼š

![](./md_png/Picture1.png)


æ•´ä½“çš„åˆ†ææ­¥éª¤å¦‚ä¸‹ï¼š

```mermaid
graph TD;
 æ–‡æœ¬-->æ–‡æœ¬é¢„å¤„ç†;
 æ–‡æœ¬é¢„å¤„ç†-->è¯­ç§åˆ¤åˆ«; 
è¯­ç§åˆ¤åˆ«-->åˆ†è¯-è¯æ€§æ ‡æ³¨;
 åˆ†è¯-è¯æ€§æ ‡æ³¨-->åˆ†å—åˆ‡è¯ & è¯å½¢è¿˜åŸ & è¯å¹²æå–;
åˆ†å—åˆ‡è¯ & è¯å½¢è¿˜åŸ & è¯å¹²æå–-->ç»“æœè¿‡æ»¤ ;
```

## æ–‡æœ¬é¢„å¤„ç†

å»é™¤è¡¨æƒ…å’Œå¤šä½™çš„ç©ºæ ¼ï¼Œå¤§å°å†™è½¬åŒ–ç­‰ä¸€ç³»åˆ—å¸¸è§„æ“ä½œã€‚

```python
import emoji
import re

def emoji_remove(text):
    text = emoji.demojize(text)
    res = re.sub(':\S+?:', ' ', text)
    return res

def text_clean(text):
    text = re.sub(r"[ ]+", " ", text)
    text = self.emoji_remove(text)
    text = text.strip().lower()
    return text
```

## è¯­ç§åˆ¤åˆ«

ä½¿ç”¨`langid`å·¥å…·è¿›è¡Œè¯­ç§åˆ¤åˆ«ï¼Œ`langid`æ”¯æŒ56ç§è¯­è¨€è¯†åˆ«ï¼Œå’Œæ¦‚ç‡è¾“å‡ºï¼Œæ•´ä½“è¯†åˆ«æ•ˆæœè¿˜ä¸é”™ï¼Œä¹Ÿæ¯”è¾ƒç¨³å®šã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯æ ‡ç‚¹ç¬¦å·ï¼ˆåŠè§’ï¼Œå…¨è§’ï¼‰ä¼šå½±å“åˆ¤åˆ«ï¼Œæ‰€ä»¥åœ¨åˆ¤åˆ«çš„æ—¶å€™å¯ä»¥å…ˆå°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢æˆç©ºæ ¼ã€‚

```python
from langid.langid import LanguageIdentifier, model
identifier = LanguageIdentifier.from_modelstring(model,norm_probs=True)
identifier.classify('æˆ‘çˆ±ä½ ä¸­å›½')
```

> ('zh', 0.9999998421995415)

## åˆ†å¥ & åˆ†è¯

`nltk`é»˜è®¤ç”¨`PunktSentenceTokenizer`åˆ†å¥ï¼Œç”¨åŸºäºå®¾å·æ ‘åº“åˆ†è¯è§„èŒƒçš„`TreebankWordTokenizer`åˆ†è¯ã€‚ç®€å•å¯¹æ¯”`nltk.word_tokenize`å’Œ`nltk.wordpunct_tokenize`çš„åŒºåˆ«ï¼š

* word_tokenizeï¼ˆé»˜è®¤ï¼‰ï¼š
  
  * ç¼©å†™(contraction)]()ï¼šIsnâ€™tä¼šåˆ†æˆIså’Œnâ€™t
  * ç¬¦å·ç²˜è¿ï¼š19+ï¼Œhelloï½è¿™ç§ä¼´æœ‰ç¬¦å·çš„è¯ä¼šé»˜è®¤å°†ä»–ä»¬åˆ’åˆ†æˆä¸€ä¸ªæ•´ä½“
  
* wordpunct_tokenizeï¼š
  
  * ç¼©å†™(contraction)ï¼šIsnâ€™tä¼šåˆ†æˆIsnå’Œt
  * ç¬¦å·ç²˜è¿ï¼š 19+ï¼Œhelloï½è¿™ç§ä¼´æœ‰ç¬¦å·çš„è¯ä¼šåˆ’åˆ†æˆ19,+,hello,~


```python
import nltk
print(nltk.word_tokenize("isn't"))
print(nltk.wordpunct_tokenize("isn't"))
print(nltk.word_tokenize('19~'))
print(nltk.wordpunct_tokenize('19~'))
```

> ['is', "n't"]
>
> ['isn', "'", 't']
>
> ['19~']
>
> ['19', '~']

æ€»çš„æ¥è¯´ï¼Œ`wordpunct_tokenize`çš„é¢—ç²’åº¦æ›´ç»†ï¼Œä½†æ˜¯ä¹Ÿå®¹æ˜“åˆ†é”™è¯ã€‚	`word_tokenize`é™¤äº†ç¬¦å·ç²˜è¿å¤„ç†ä¸å¥½ä¹‹å¤–ï¼Œæ•ˆæœæ›´å¥½ã€‚åŒæ—¶`NLTK`å·¥å…·æ”¯æŒé€‰æ‹©æŒ‡å®šåˆ†è¯å™¨ï¼Œå¯ä»¥é€šè¿‡`dir(nltk.tokenize)` è¿›è¡Œé€‰æ‹©ã€‚


## é™„ä»¶

![](./md_png/Picture4.png)

## å‚è€ƒ

[è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ä¹‹NLTK](https://www.biaodianfu.com/nltk.html)

[ Natural Language Toolkit](https://www.nltk.org)

æœ€æ–°æ›´æ–°äº 2021.12.29

