# è‹±æ–‡NLPå®è·µã€NLTKã€‘

åœ¨**æµ·å¤–èˆ†æƒ…ç›‘æ§**é¡¹ç›®çš„å¥‘æœºä¸‹ï¼Œé€šè¿‡ä¸æ–­çš„é‡åˆ°é—®é¢˜ï¼Œå†è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ï¼Œå‘ç°äº†ä¸€äº›å¥½ç©çš„æ–‡æœ¬å¤„ç†æ–¹æ³•å’Œä¸€äº›å®ç”¨çš„å·¥å…·ã€‚æ‰€ä»¥å†™åœ¨è¿™é‡Œå½“ä½œåˆ†äº«çš„åŒæ—¶ï¼Œä¹Ÿå½“ä½œæ˜¯è‡ªå·±çš„ä¸€ä¸ªå¤‡å¿˜å½•ã€‚ ç”±äºæ¸¸æˆèŠå¤©æ˜¯ä¸€ä¸ªå¤šè¯­ç§çš„ç¯å¢ƒï¼Œé’ˆå¯¹ä¸åŒè¯­è¨€çš„å¤„ç†é€»è¾‘å’ŒæŠ€å·§ä¹Ÿä¸å°½ç›¸åŒã€‚ä»Šå¤©å…ˆæ€»ç»“çš„æ˜¯**è‹±æ–‡**ï¼Œåç»­ä¼šé™†é™†ç»­ç»­æ›´æ–°**ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©è¯­**ç­‰å…¶ä»–è¯­ç§ã€‚é€‰æ‹©çš„åˆ†æå·¥å…·æ˜¯`NLTK`ï¼ˆpsï¼š`NLTK`ä¸æ”¯æŒä¸­æ–‡ï¼‰ï¼Œä¸‹æ–‡ä¸­å±•ç¤ºçš„ä»£ç ä¾‹å­ä¹Ÿæ˜¯é€šè¿‡`NLTK`æ¥å®è·µçš„ã€‚

- [x] ğŸ§ **æ–‡æœ¬æŒ–æ˜çš„æ€è·¯**
- [x] ğŸ› **æ–‡æœ¬é¢„å¤„ç†**ï¼›
- [x] ğŸš®**è¯­ç§åˆ¤åˆ«**ï¼›
- [x] ğŸ”ª **åˆ†è¯**ï¼›
- [x] ğŸŒŸ**è¯æ€§æ ‡æ³¨**ï¼›
- [x] ğŸ”· **åˆ†å—åˆ‡è¯**ï¼›
- [x] ğŸ‘¶**è¯å½¢è¿˜åŸ**  & **è¯å¹²æå–**
- [x] â›³ï¸  **ç»“æœè¿‡æ»¤**

## æ–‡æœ¬æŒ–æ˜çš„æ€è·¯

åœ¨åˆ†æä¹‹å‰ï¼Œåº”å½“æœ‰æ„è¯†çš„ç»´æŠ¤ä¸€å¥—è‡ªå·±çš„**ç”¨æˆ·è¯å…¸**ï¼Œå¯ä»¥è®©è¿è¥åŒäº‹æä¾›æ¸¸æˆä¸“ä¸šåè¯ï¼Œè¿™äº›è¯ä¹Ÿå¾€å¾€æ˜¯è¿è¥åŒäº‹æ¯”è¾ƒå…³æ³¨çš„é‡ç‚¹è¯æ±‡ï¼Œè¿™äº›è¯æ±‡åœ¨å¸‚é¢ä¸Šçš„åˆ†è¯å·¥å…·ä¸­æ˜¯æ¯”è¾ƒéš¾å‡†ç¡®åˆ‡å‰²çš„ã€‚åŒæ—¶ï¼Œå¯»æ‰¾åˆé€‚çš„*stopwords***åœç”¨è¯è¯å…¸**ï¼Œå¯¹åç»­çš„åˆ†è¯ç»“æœå†åšä¸€å±‚è¿‡æ»¤ã€‚å­—å…¸æœ€å¥½æœ‰ä¸€ä¸ª**æ ‡å‡†åŒ–**çš„æ ¼å¼ï¼Œåç»­åæ–°çš„ç”¨é€”å¯ä»¥çµæ´»çš„ä½¿ç”¨ã€‚ç›®å‰æˆ‘çš„å­—å…¸æŒ‰ç…§ä¸‹é¢è¿™ä¸ªæ ¼å¼è¿›è¡Œæ•´ç†ï¼š

![](./md_png/Picture1.png)

```mermaid
graph TD;
 æ–‡æœ¬-->æ–‡æœ¬é¢„å¤„ç†;
 æ–‡æœ¬é¢„å¤„ç†-->è¯­ç§åˆ¤åˆ«; 
è¯­ç§åˆ¤åˆ«-->åˆ†è¯-è¯æ€§æ ‡æ³¨;
 åˆ†è¯-è¯æ€§æ ‡æ³¨-->åˆ†å—åˆ‡è¯ & è¯å½¢è¿˜åŸ & è¯å¹²æå–;
åˆ†å—åˆ‡è¯ & è¯å½¢è¿˜åŸ & è¯å¹²æå–-->ç»“æœè¿‡æ»¤;
```

## æ–‡æœ¬é¢„å¤„ç†

å»é™¤è¡¨æƒ…å’Œå¤šä½™çš„ç©ºæ ¼ï¼Œå¤§å°å†™è½¬åŒ–ç­‰ä¸€ç³»åˆ—å¸¸è§„æ“ä½œã€‚

```python
import emoji
import re

def emoji_remove(text):
    text = emoji.demojize(text)
    res = re.sub(':\S+?:', ' ', text)
    return res

def text_clean(text):
    text = re.sub(r"[ ]+", " ", text)
    text = self.emoji_remove(text)
    text = text.strip().lower()
    return text
```

## è¯­ç§åˆ¤åˆ«

ä½¿ç”¨`langid`å·¥å…·è¿›è¡Œè¯­ç§åˆ¤åˆ«ï¼Œ`langid`æ”¯æŒ56ç§è¯­è¨€è¯†åˆ«ï¼Œå’Œæ¦‚ç‡è¾“å‡ºï¼Œæ•´ä½“è¯†åˆ«æ•ˆæœè¿˜ä¸é”™ï¼Œä¹Ÿæ¯”è¾ƒç¨³å®šã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯æ ‡ç‚¹ç¬¦å·ï¼ˆåŠè§’ï¼Œå…¨è§’ï¼‰ä¼šå½±å“åˆ¤åˆ«ï¼Œæ‰€ä»¥åœ¨åˆ¤åˆ«çš„æ—¶å€™å¯ä»¥å…ˆå°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢æˆç©ºæ ¼ã€‚

```python
from langid.langid import LanguageIdentifier, model
identifier = LanguageIdentifier.from_modelstring(model,norm_probs=True)
identifier.classify('æˆ‘çˆ±ä½ ä¸­å›½')
```

> ('zh', 0.9999998421995415)

## åˆ†å¥ & åˆ†è¯

`nltk`é»˜è®¤ç”¨`PunktSentenceTokenizer`åˆ†å¥ï¼Œç”¨åŸºäºå®¾å·æ ‘åº“åˆ†è¯è§„èŒƒçš„`TreebankWordTokenizer`åˆ†è¯ã€‚ç®€å•å¯¹æ¯”`nltk.word_tokenize`å’Œ`nltk.wordpunct_tokenize`çš„åŒºåˆ«ï¼š

* word_tokenizeï¼ˆé»˜è®¤ï¼‰ï¼š
  
  * ç¼©å†™(contraction)]()ï¼šIsnâ€™tä¼šåˆ†æˆIså’Œnâ€™t
  * ç¬¦å·ç²˜è¿ï¼š19+ï¼Œhelloï½è¿™ç§ä¼´æœ‰ç¬¦å·çš„è¯ä¼šé»˜è®¤å°†ä»–ä»¬åˆ’åˆ†æˆä¸€ä¸ªæ•´ä½“
  
* wordpunct_tokenizeï¼š
  
  * ç¼©å†™(contraction)ï¼šIsnâ€™tä¼šåˆ†æˆIsnå’Œt
  * ç¬¦å·ç²˜è¿ï¼š 19+ï¼Œhelloï½è¿™ç§ä¼´æœ‰ç¬¦å·çš„è¯ä¼šåˆ’åˆ†æˆ19,+,hello,~


```python
import nltk
print(nltk.word_tokenize("isn't"))
print(nltk.wordpunct_tokenize("isn't"))
print(nltk.word_tokenize('19~'))
print(nltk.wordpunct_tokenize('19~'))
```

> ['is', "n't"]
>
> ['isn', "'", 't']
>
> ['19~']
>
> ['19', '~']

æ€»çš„æ¥è¯´ï¼Œ`wordpunct_tokenize`çš„é¢—ç²’åº¦æ›´ç»†ï¼Œä½†æ˜¯ä¹Ÿå®¹æ˜“åˆ†é”™è¯ã€‚	`word_tokenize`é™¤äº†ç¬¦å·ç²˜è¿å¤„ç†ä¸å¥½ä¹‹å¤–ï¼Œæ•ˆæœæ›´å¥½ã€‚åŒæ—¶`NLTK`å·¥å…·æ”¯æŒé€‰æ‹©æŒ‡å®šåˆ†è¯å™¨ï¼Œå¯ä»¥é€šè¿‡`dir(nltk.tokenize)` è¿›è¡Œé€‰æ‹©ã€‚

## è¯æ€§æ ‡æ³¨

`nltk.pos_tag`æ”¯æŒå¯¹åˆ†å¥½çš„tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ã€‚è¯æ€§å¤§ç±»å¦‚ä¸‹å›¾æ€»ç»“ï¼Œé‡ç‚¹å…³æ³¨åè¯**NN***ï¼Œå½¢å®¹è¯**JJ***ï¼ŒåŠ¨è¯**V***ï¼Œå‰¯è¯**RB***ã€‚

![](./md_png/Picture2.png)

## åˆ†å—åˆ‡è¯

åŸºäºä¸Šè¿°çš„åˆ†è¯å’Œè¯æ€§æ ‡æ³¨ï¼Œç»“åˆå¥æ³•åˆ†æç¼–å†™**è¯­æ³•è¡¨è¾¾å¼**ï¼Œè¿›è¡Œ`chunk`åˆ’åˆ†ã€‚`nltk.regexpParser`è§£æè¯­æ³•è¡¨è¾¾å¼ï¼Œå¹¶ä»¥`è¯­æ³•æ ‘`è¿›è¡Œåˆ†æå±•ç¤ºã€‚ä¸‹é¢çš„ä¾‹å­ä½¿ç”¨äº†å››ä¸ªè‹±æ–‡çš„è¯­æ³•ç»“æ„ï¼š

* Adj-Nï¼šå½¢å®¹è¯+åè¯
* Adv-Vï¼šå‰¯è¯+åè¯
* V-Nï¼šåŠ¨è¯+ï¼ˆå† è¯ï¼‰+åè¯
* Nounï¼šåè¯+ï¼ˆä»‹è¯|æ‰€æœ‰æ ¼ï¼‰+åè¯


ä»£ç ä¸­è¯¦ç»†ä¸¾ä¾‹ï¼Œå¤§å®¶ä¹Ÿå¯è‡ªè¡Œç¼–å†™è‡ªå·±å…³æ³¨çš„è¯­æ³•ç»“æ„è¡¨è¾¾å¼ã€‚

```python
from nltk import RegexpParser
from nltk import word_tokenize, pos_tag

def chunk_split(tokens): # --> [(word,tag),....]
    # chunk + fitter
    # POS tag a sentence
    res = []
    grammar = """
    Adj-N: {<JJ.*>+<NN.*>+}
    Adv-V: {<RB.*>+<V.*>+}
    V-N: {<V.*>+<DT>?<NN.*>+} # want some water/want apple --> verb + determiner + noun/verb + noun
    Noun: {<NN.*>+<IN|POS>?<NN.*>+} # --> apple of eva/office lady --> noun + in + noun/noun + noun
    """
parser = RegexpParser(grammar)
chunks = parser.parse(tokens)
```

```python
str0 = "Hello World! Isn't it good to see you? Thanks for buying this book."
sent_cut = nltk.pos_seg(nltk.word_tokenize(str0))
chunk_res = chunk_split(sent_cut)
print(chunk_res)
```

> [('hello world', 'cu'), ('buy this book', 'cu')]

![](./md_png/Picture3.png)

å¯ä»¥çœ‹åˆ°`hello world`å’Œ'buy this book`çŸ­è¯­æœ‰åˆ’åˆ†å‡ºæ¥ã€‚

## è¯å½¢è¿˜åŸ & è¯å¹²æå–

è‹±æ–‡åœ¨ä¸åŒçš„è¯­æ³•ç¯å¢ƒä¸­ï¼Œè¯æœ‰å„ç§å„æ ·çš„å˜å½¢ï¼šå•å¤æ•°ï¼Œæ¯”è¾ƒçº§ï¼Œè¿‡å»åˆ†è¯ï¼ŒåŠ¨åè¯ç­‰ç­‰ã€‚ä¸ºäº†ç²¾ç¡®ç»Ÿè®¡é«˜é¢‘è¯ï¼Œæˆ‘ä»¬éœ€è¦å°†è¯è¿›è¡Œè¯å½¢è¿˜åŸæˆ–è€…æ˜¯è¯å¹²æå–ã€‚

a.     è¯å½¢è¿˜åŸ`nltk.stem. WordNetLemmatizer`ï¼šå¯ä»¥æ ¹æ®æŒ‡å®šçš„è¯æ€§è¿›è¡Œè¿˜åŸ

```python
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize('cooking', pos='v'))
print(lemmatizer.lemmatize('cookbooks',pos=â€™nâ€™))
```

> cook
>
> cookbook

b.     è¯å¹²æå–`nltk.stem.PorterStemmer`:

```
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

print(stemmer.stem('working'))
print(stemmer.stem('worked'))
```

> work
> 
> work

## ç»“æœè¿‡æ»¤

åœ¨æœ€åè¾“å‡ºåˆ†æç»“æœä¹‹å‰ï¼Œå†ç»“åˆä»¥ä¸‹æ–¹å¼è¿‡æ»¤ç»“æœï¼š

a.     **è¯æ€§è¿‡æ»¤**ï¼šè¿‡æ»¤ä¸€äº›ä¸å…³æ³¨çš„è¯æ€§

b.    **å•è¯è¿‡æ»¤**ï¼šé•¿åº¦ä¸º1çš„è¯

c.     **åœç”¨è¯è¿‡æ»¤**

## é™„ä»¶

![](./md_png/Picture4.png)

## å‚è€ƒ

[è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ä¹‹NLTK](https://www.biaodianfu.com/nltk.html)

[ Natural Language Toolkit](https://www.nltk.org)

æœ€æ–°æ›´æ–°äº 2021.12.29
